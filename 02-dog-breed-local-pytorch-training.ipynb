{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate our fundamental setup for training on the presented Dataset with PyTorch. Because lots of this code is later 'hidden away' from the enduser in SageMaker, it is good, to get a first impression about what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import PIL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended the last noetbook (exploration) with lots of insights how the preprocessing is done in this project. While there are plenty python libraries for image manipulation, we should always focus on the compability with our target framework - that is PyTorch on AWS Sagemaker.\n",
    "Hence I will demonstrate here, how the `imgaug` library and the PyTorch `torchvision.transforms` library play together and can be used for the later training steps.\n",
    "First lets install `imgaug` again and define a transform object that does some sequential operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aleju/imgaug\n",
      "  Cloning https://github.com/aleju/imgaug to /tmp/pip-req-build-mbxda9hs\n",
      "Requirement already satisfied (use --upgrade to upgrade): imgaug==0.2.9 from git+https://github.com/aleju/imgaug in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (1.1.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (1.15.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (1.11.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (2.3.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (5.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (3.0.3)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (1.6.4.post2)\n",
      "Requirement already satisfied: opencv-python-headless in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug==0.2.9) (4.1.0.25)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.9) (2.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.9) (0.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug==0.2.9) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug==0.2.9) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug==0.2.9) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug==0.2.9) (2.7.3)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug==0.2.9) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug==0.2.9) (39.1.0)\n",
      "Building wheels for collected packages: imgaug\n",
      "  Running setup.py bdist_wheel for imgaug ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-j8oodfiz/wheels/9c/f6/aa/41dcf2f29cc1de1da4ad840ef5393514bead64ac9e644260ff\n",
      "Successfully built imgaug\n",
      "\u001b[31mfastai 1.0.52 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aleju/imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAugmentation:\n",
    "  def __init__(self):\n",
    "    self.aug = iaa.Sequential([\n",
    "        iaa.Resize((224, 224)),\n",
    "        # blur images with a sigma of 0 to 3.0\n",
    "        iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "        # horizontally flip 50% of the images\n",
    "        iaa.Fliplr(0.5),\n",
    "        # rotate by -20 to +20 degrees. The default mode is 'constant' which displays a constant value where the\n",
    "        # picture was 'rotated out'. A better mode is 'symmetric' which \n",
    "        #'Pads with the reflection of the vector mirrored along the edge of the array' (see docs) \n",
    "        iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
    "        # do edge detection on 25% of the pictures\n",
    "        iaa.Sometimes(0.25,\n",
    "                      iaa.EdgeDetect(alpha=(0.5, 1.0))),\n",
    "    ])\n",
    "      \n",
    "  def __call__(self, img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    '''\n",
    "    # This is a experimental try to get even more shape\n",
    "    # informations via imgaug heatmap feature.\n",
    "    # While this code works, I could not find out how\n",
    "    # to create custom heatmaps (not the quoakka_heatmap) for our images\n",
    "\n",
    "    heatmap = ia.quokka_heatmap(size=0.25)\n",
    "    print(f'img shape via heatmap.shape {heatmap.shape}')\n",
    "    print(f'img shape {img.shape}')\n",
    "    image_aug, heatmap_aug = self.aug(image=img, heatmaps=heatmap)\n",
    "    return np.hstack([image_aug, heatmap_aug.draw(cmap=\"gray\")[0]])\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # either return a PIL.Image here or use PyTorch Lambda transform later on\n",
    "    # return PIL.Image.fromarray(self.aug.augment_image(img))    \n",
    "    return self.aug.augment_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch makes it very easy for us to specify a transformation pipeline. This happens with `torchvision.transforms`. The Compose object accepts different torchvision.transforms steps, that will be executed in sequence. To include our custom `imgaug` preprocessing step, we transfer the output of `AdvancedAugmentation` (that is `self.aug.augment_image(img)`). This needs to be done, because the library method `augment_image` returns an `ndarray`, that the following `torchvision.transforms` Transformers can not pick up. Hence I included a transform step, that picks up this `ndarray` and converts it back to an Image again: `torchvision.transforms.Lambda(lambda x: PIL.Image.fromarray(x))`.\n",
    "Regarding the `valid` and `test` transforms, we don't want to manipulate the original images to much only resize (that is important, so our CNN can pick it up) and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        # this is where our imgaug transformations get included\n",
    "        AdvancedAugmentation(),\n",
    "        torchvision.transforms.Lambda(lambda x: PIL.Image.fromarray(x)),\n",
    "        torchvision.transforms.RandomGrayscale(p=0.3),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the data_transforms, we create a smaller dogImages folder that suits our purposes for this that notebook - that is to simply demonstratet the learning pipeline (the whole dataset leads to `No space left on device` errors hence the real data should be executed on a separate training container - like it will be done in later notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/cnn-classifier\n",
      "mkdir: cannot create directory ‘dogImages_local_training’: No space left on device\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir dogImages_local_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the specification of our data transformers for each dataset `train`, `valid` and `test`, we create the corresponding datasets & dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dogImages'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'valid','test']}\n",
    "\n",
    "# ATTENTION: WE NEED TO SET num_workers to 0! Otherwise we will run into errors\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'valid', 'test']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up our data loaders the next common step is to specify our training and test scripts that will train our PyTorch model on the data. The following scripts are very similar to the ones we learned during the course, so I will not comment so much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data, target in dataloaders['train']:\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for data, target in dataloaders['valid']:\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "    \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(dataloaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(dataloaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "         # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project proposal we commited on using an advanced deep learning technique called transfer learning. In the next cells I demonstrate how this can be leveraged in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vgg16 model\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vgg16\n",
    "n_inputs = model.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, 133)\n",
    "model.classifier[6] = last_layer\n",
    "\n",
    "# set the cuda flag\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.010934 \tValidation Loss: 2.289984\n",
      "Validation loss decreased (inf --> 2.289984).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.844379 \tValidation Loss: 1.430140\n",
      "Validation loss decreased (2.289984 --> 1.430140).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.353899 \tValidation Loss: 1.086405\n",
      "Validation loss decreased (1.430140 --> 1.086405).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.133385 \tValidation Loss: 0.911726\n",
      "Validation loss decreased (1.086405 --> 0.911726).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# Fix for cuda error resulting from truncated images\n",
    "# https://stackoverflow.com/a/23575424/7434289\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "model = train(5, dataloaders, model, optimizer, criterion, use_cuda, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally it is time to measure the performance of our model via accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataloaders, model, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
